{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513eb273-42e0-49bf-9f5a-7102222d8743",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center; font-weight:bold\">Understanding Transforms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66a923-adb4-4d2a-91eb-0570a633ff24",
   "metadata": {},
   "source": [
    "### TorchVision datasets have two parameters \n",
    "### - transform: to modify the features\n",
    "### - target_transform:  to modify the labels - that accept callables containing the transformation logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef000669-9d5d-41f7-bcb3-26d647eaa255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our FashionMNIST data, the features are in PIL image format and the labels are integers.\n",
    "# To be able to train our model, we need the features as normalize tensors (mean of zero and standard deviation of 1 called Z-score normalization)\n",
    "# We also need the labels to be one-hot-encoded tensors (binary value of zeroes with only 1 a the position of the labels index in the list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f312c92-c877-4898-80b3-df606dd790fd",
   "metadata": {},
   "source": [
    "## Normalized Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f54d3a-29c3-4a70-9b60-faf586cb7abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2649, -0.6325,  0.0000,  0.6325,  1.2649])\n"
     ]
    }
   ],
   "source": [
    "# A normalized tensor refers to a tensor whose values have been transformed to a specific range, distribution, or scale to improve the stability and performance of machine learning models. \n",
    "# The goal of normalization is to make sure that the data is centered around a particular value (often 0) and has a consistent spread, which can help models converge faster during training.\n",
    "# In this lesson, we will be use the Mean-Variance (Z-Score normilization)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Original tensor\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "# Z-score normalization\n",
    "mean, std = tensor.mean(), tensor.std()\n",
    "normalized_tensor = (tensor - mean) / std\n",
    "print(normalized_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42dea7c-e08b-4139-ab43-5a47b1d26dae",
   "metadata": {},
   "source": [
    "## One-Hot-Encoded Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf74221-27c0-42a1-a98b-2c70f0bd3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To achieve one-hot encoded tensor from our classes, which has a size of 10, we would create a binary value for each lable and initialize all the bits to zero\n",
    "# Then for each binary value, we would set one bit to zero.\n",
    "# The bit to set to zero is the bit in the position corresponsing to the index of that item in the list\n",
    "\n",
    "labels = ['cat', 'dog', 'chicken']\n",
    "labels_tensor = [100, 010, 001]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0029e-8c9d-49fd-b54b-ddf1cd80cce4",
   "metadata": {},
   "source": [
    "## Transforming Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33639378-e73d-4c01-ac7b-cdcc16688a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to root/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:07<00:00, 3567487.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting root/FashionMNIST/raw/train-images-idx3-ubyte.gz to root/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to root/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 563888.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting root/FashionMNIST/raw/train-labels-idx1-ubyte.gz to root/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to root/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:02<00:00, 1555746.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting root/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to root/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to root/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 9223527.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting root/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to root/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the transforms and target_transforms parameter we an achieve both normalization and encoding\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"root\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # Converts a PIL image or NumPy ndarray into a FloatTensor and scales the image's pixel intensity values in the range of [0., 1.]\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a84ecf-e4ac-48db-bf5c-37d3edb5c307",
   "metadata": {},
   "source": [
    "### Lambda Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e407d93-c35a-4d7c-aee0-44268d3cf37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned earlier, the target_transform parameter takes a lambda funtions that performs a transformation on the labels\n",
    "# In this example we are provide a function that performs one-hot-encoding.\n",
    "# It first creates a zero tensor of size 10 (number of labels in our dataset)\n",
    "# Next it calls call the scatter_ function which assigns a value of 1 to the index given by the label y\n",
    "\n",
    "target_transform = Lambda(\n",
    "    lambda y: torch.zeros(10, dtype=torch).scatter(dim=0, index=torch.tensor(y), value=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1051451-d360-4497-9f2b-395e5ed7bbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
