{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d577d075-07c8-4e39-a0a4-9a53e453760a",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center; font-weight:bold\">Build a Simple Neural Netework</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720d214-7d29-419b-93b7-905cd143918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this module, we would build a simple neural network made up of 3 layers:\n",
    "### 1. one input layer, a 28x28(input) by 512(output) dimension\n",
    "### 2. one hiddel layer, a 512(input) by 512(output) dimension layer, activated with the ReLU activation function\n",
    "### 3. one output layer, a 512(input) by 10(output) diminsion layer, activate with the ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f19e7-1375-4aef-bc84-fa8ae20ec1aa",
   "metadata": {},
   "source": [
    "## Import Neccessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1596bec2-8c89-4b0b-99e4-78853b6aa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0d5c2-1bcd-4d2e-879c-83658f920a27",
   "metadata": {},
   "source": [
    "## Obtain the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e42059-dcb6-4b51-9c77-904d5fae8853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "### you already know, one of the properties of a tensor is the device on which it resides\n",
    "### this could be either a CPU or a GPU\n",
    "### Generally, we would prefer to use a GPU it is available\n",
    "\n",
    "# Device selection logic\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0843123-1622-4ac3-a770-773eb9d7bc0d",
   "metadata": {},
   "source": [
    "## Define the Class (extend nn.Module class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf30a5-eaeb-40e9-9839-665cb6f8ef64",
   "metadata": {},
   "source": [
    "#### To create neural network, we must extend the nn.Module class. We would also need to implement the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679b0d4a-2777-4375-bfad-2d7a88163381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31080c-cb48-4851-920d-3c9183946a7b",
   "metadata": {},
   "source": [
    "#### Having created our Neural Network model,let's intantiate it and move it to a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7701e2f7-adfa-461f-8f4e-03e331341a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6b084-1f59-4c7c-b5d5-42479c71acf6",
   "metadata": {},
   "source": [
    "## Making Prediction Using Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac74a2b-ffc1-43e3-bbad-d61336beb2c6",
   "metadata": {},
   "source": [
    "#### To make prediction using our model, we would need to pass some input data to our model\n",
    "#### This would return a 2-dimensional tensor with dim=0 corresponding to the raw ouput and dimention 1 corresponding to the output value.\n",
    "#### Each of the output is one of 10 predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966317c-e761-477b-a98d-c463098c0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device= device)\n",
    "logits = model(X)\n",
    "prediction_probabilities = nn.Softmax(dim=1)(logits)\n",
    "y_predicted = prediction_probabilities.argmax(1)\n",
    "print(f\"Predicted class {y_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5843e-fc4f-458a-aecb-c95097bb8e02",
   "metadata": {},
   "source": [
    "## Understanding the Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03def12-8ff0-42d7-864e-f7cd486dfb2e",
   "metadata": {},
   "source": [
    "#### Let's now take some time to understand the model layers.\n",
    "#### Let's take a batch of 3 images of size 28  x 28 and pass it through the layers of the network and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b62f9b8-35d7-40a3-82a2-b10e33ae6c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c374f-79f2-41bb-895d-87dd660584ba",
   "metadata": {},
   "source": [
    "### nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271eb7d-d755-4248-9bcb-001b92f2b7c3",
   "metadata": {},
   "source": [
    "#### This layer would take the 2-dimentional 28 x 28 image and convert it into a 1-dimentional array of lenght 784 (28 x 28)\n",
    "#### The dimension is maintained at dim=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee1f4a2f-ff07-41d0-8901-98cbc416b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5d278-dda5-425e-9e77-03fe73a9543d",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c0291-7391-4beb-ba19-5ce95905d3cd",
   "metadata": {},
   "source": [
    "#### The linear layer will apply a linear transformation on the input layer using it's stored weights and biases.\n",
    "#### For example, given x, we apply a function of x, f(x) = x.weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "600ce9d0-e8d3-4aa0-9794-5b7c149aca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302119bd-0612-49b1-83f1-f70d5626091f",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c680842-b3f4-4021-af69-44e31de64517",
   "metadata": {},
   "source": [
    "#### This is a non-linear activation function that is applied after the linear transformation  to introduce non-linearity.\n",
    "#### This makes it possible for the neural network to learn a from a wider variety of input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724d1b6-ee28-4a00-90ff-c7af8f44cce4",
   "metadata": {},
   "source": [
    "#### Although we use ReLU here, there are other activation functions that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "876b7ed4-1140-4478-ba1f-bf9f0f04f62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying ReLU: tensor([[ 7.5765e-02,  5.7657e-01, -9.2276e-02,  7.9460e-02,  4.2622e-01,\n",
      "          4.1913e-01,  2.0428e-01,  2.7230e-01, -3.7125e-02,  6.6380e-02,\n",
      "         -4.6995e-02,  2.8898e-01, -2.9127e-01, -3.4531e-01, -5.0734e-02,\n",
      "         -5.8721e-04,  2.9879e-01, -1.7054e-01,  2.3635e-01, -2.2674e-01],\n",
      "        [-2.9477e-01,  4.8444e-01, -3.2404e-01, -2.5243e-01,  6.1391e-01,\n",
      "          8.0211e-02,  2.2512e-01,  4.2206e-01, -4.6083e-01,  1.4280e-01,\n",
      "         -2.7107e-01,  1.4749e-01, -5.0821e-01, -1.9676e-01, -2.1479e-01,\n",
      "          1.8214e-01,  1.1709e-01, -5.4246e-03, -4.5451e-02, -6.3239e-01],\n",
      "        [-2.9733e-01,  4.1945e-01,  7.8776e-03, -4.2119e-01,  5.2320e-01,\n",
      "          2.7397e-01,  2.6396e-01,  4.9650e-01, -1.5880e-01,  9.2246e-03,\n",
      "         -3.3053e-01,  2.3142e-01, -5.4229e-01,  6.7973e-02, -2.1788e-01,\n",
      "          1.6566e-02,  7.7404e-01, -1.3827e-01,  2.0108e-01, -4.8575e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After applying ReLU: tensor([[0.0758, 0.5766, 0.0000, 0.0795, 0.4262, 0.4191, 0.2043, 0.2723, 0.0000,\n",
      "         0.0664, 0.0000, 0.2890, 0.0000, 0.0000, 0.0000, 0.0000, 0.2988, 0.0000,\n",
      "         0.2363, 0.0000],\n",
      "        [0.0000, 0.4844, 0.0000, 0.0000, 0.6139, 0.0802, 0.2251, 0.4221, 0.0000,\n",
      "         0.1428, 0.0000, 0.1475, 0.0000, 0.0000, 0.0000, 0.1821, 0.1171, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.4195, 0.0079, 0.0000, 0.5232, 0.2740, 0.2640, 0.4965, 0.0000,\n",
      "         0.0092, 0.0000, 0.2314, 0.0000, 0.0680, 0.0000, 0.0166, 0.7740, 0.0000,\n",
      "         0.2011, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before applying ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After applying ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca42be-7493-4c49-bbb7-30d9ec8359a9",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1a69e-f135-4d5a-8716-b05963f7cdd4",
   "metadata": {},
   "source": [
    "#### nn.Sequential is an ordered container of modules.\n",
    "#### Data is passed through all the modules in the same order as you defined it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb347a0-819e-45e3-ac58-24621994863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpanalyticsolution.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102f219-ac02-4699-a99a-43b9e8a4c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We an also build the network like like this using nn.Sequential\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU,\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158ffcd-da92-43a7-b66d-d6a509f6bf27",
   "metadata": {},
   "source": [
    "### nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7a134-10eb-447e-95d0-e760e1f06a8c",
   "metadata": {},
   "source": [
    "#### The last layer of the neural network and it returns logits, that is raw values in [-inf, inf]\n",
    "#### The output of this last layer is passed into the Softmax module\n",
    "#### The logits are scaled to [0,1] which represents the model's predicted probabilities for each class\n",
    "#### The dim parameter indicates the dimension along which the values must sum up to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55a906fb-01cc-4de1-9e4f-a989adcce270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0939, 0.0967, 0.0986, 0.0981, 0.1033, 0.1065, 0.1021, 0.1024, 0.0993,\n",
       "         0.0990]], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "predicted_probabilities = softmax(logits)\n",
    "print(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3be38-2b78-4e84-8180-7d625d15bd56",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c8e29-79a7-4d46-8328-5b9245062f39",
   "metadata": {},
   "source": [
    "#### Many layers inside a neural network are parameterized.\n",
    "#### This means that they have associated weights and biases that are optimized during training\n",
    "#### Extending the nn.Module class automatically tracks all the fields defined inside your model object \n",
    "#### and makes all parameters accessible using your model's parameters() or named_parameters() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f7269e-a5e4-4df7-9d2c-d0d79f511549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[-1.7760e-02, -9.9057e-03,  2.0733e-02,  ...,  4.2260e-05,\n",
      "         -3.2030e-02,  1.6735e-02],\n",
      "        [ 3.4041e-02, -5.3650e-03,  2.0612e-02,  ...,  1.4829e-02,\n",
      "          5.5902e-03, -1.0065e-02]], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([ 0.0299, -0.0093], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[ 0.0178, -0.0357, -0.0225,  ...,  0.0375,  0.0302, -0.0233],\n",
      "        [-0.0431, -0.0338,  0.0087,  ...,  0.0290,  0.0135,  0.0370]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0282, -0.0323], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[-0.0119,  0.0045,  0.0083,  ...,  0.0217,  0.0266,  0.0241],\n",
      "        [ 0.0079,  0.0242,  0.0249,  ..., -0.0233, -0.0242,  0.0206]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([ 0.0074, -0.0242], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35793fb-a06b-4b0f-8371-5e4426f6ddce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
